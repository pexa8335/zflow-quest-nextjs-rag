import concurrent.futures
import time
from threading import Lock
from ddgs import DDGS
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
import os
from dotenv import load_dotenv


class HueChatbot:
    def __init__(self, api_key=None):
        """Initialize Hue Chatbot with API key and configuration"""
        # Load environment variables
        load_dotenv()

        # Get API key from parameter or environment
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError(
                "API key not found. Please set GEMINI_API_KEY in .env file or pass as parameter."
            )

        # Configure the Gemini API with your key
        genai.configure(api_key=self.api_key)

        # Initialize the Generative Model with a system instruction
        self.model = genai.GenerativeModel(
            model_name="models/gemini-2.5-flash",
            system_instruction="""
            B·∫°n l√† h∆∞·ªõng d·∫´n vi√™n du l·ªãch chuy√™n nghi·ªáp v·ªÅ vƒÉn h√≥a Hu·∫ø. 
            H√£y tr·∫£ l·ªùi c√°c c√¢u h·ªèi m·ªôt c√°ch t·ª± nhi√™n, th√¢n thi·ªán v√† ch√≠nh x√°c d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p trong t√†i li·ªáu tham kh·∫£o. 
            Lu√¥n gi·ªØ vai tr√≤ l√† m·ªôt chuy√™n gia am hi·ªÉu v·ªÅ Hu·∫ø, lo·∫°i b·ªè c√°c th√¥ng tin th∆∞∆°ng m·∫°i, b√°n h√†ng gi√° c·∫£ n·∫øu ng∆∞·ªùi d√πng kh√¥ng y√™u c·∫ßu.
            Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch v√† t·∫≠p trung v√†o vƒÉn h√≥a, l·ªãch s·ª≠, ·∫©m th·ª±c v√† c√°c ƒëi·ªÉm du l·ªãch n·ªïi ti·∫øng c·ªßa Hu·∫ø.""",
        )

        # Configuration from environment
        self.max_search_results = int(os.getenv("MAX_SEARCH_RESULTS", 2))
        self.max_history = int(os.getenv("MAX_HISTORY", 5))
        self.request_timeout = int(os.getenv("REQUEST_TIMEOUT", 5))

        # Chat history management
        self.history = []

        # Search cache for performance
        self.search_cache = {}
        self.cache_lock = Lock()

        # Requests session for better performance
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
        )

    def _search_duckduckgo(self, query, max_results=None):
        """Search DuckDuckGo with caching for better performance"""
        if max_results is None:
            max_results = self.max_search_results

        with self.cache_lock:
            if query in self.search_cache:
                print("üöÄ D√πng cache")
                return self.search_cache[query]

        try:
            with DDGS() as ddgs:
                search_query = f"{query} Hu·∫ø, Vi·ªát Nam"
                results = list(
                    ddgs.text(search_query, max_results=max_results * 2, region="vn-vi")
                )
                urls = []
                for r in results:
                    url = r["href"]
                    if not any(
                        bad in url.lower()
                        for bad in [
                            ".pdf",
                            "facebook",
                            "youtube",
                            "instagram",
                            "shopee",
                            "tiki",
                        ]
                    ):
                        urls.append(url)
                    if len(urls) >= max_results:
                        break

                with self.cache_lock:
                    self.search_cache[query] = urls
                return urls
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói t√¨m ki·∫øm: {e}")
            return []

    def _extract_article_fast(self, url, timeout=3):
        """Extract article content using requests + BeautifulSoup - FAST"""
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")
            for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
                script.decompose()
            content_selectors = [
                "article",
                ".article-content",
                ".content",
                ".post-content",
                ".entry-content",
                ".main-content",
                "main",
                ".article-body",
            ]
            text = ""
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    text = content.get_text(strip=True, separator=" ")
                    break
            if not text or len(text) < 100:
                text = soup.get_text(strip=True, separator=" ")
            if len(text) > 2000:
                text = text[:2000] + "..."
            return text
        except Exception as e:
            # print(f"‚ö†Ô∏è L·ªói {url[:30]}...: {str(e)[:20]}...")
            return ""

    def _process_urls_parallel(self, urls):
        """Process multiple URLs in parallel for better performance"""
        combined_text = ""
        successful_count = 0
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            future_to_url = {
                executor.submit(self._extract_article_fast, url, 3): url for url in urls
            }
            try:
                for future in concurrent.futures.as_completed(future_to_url, timeout=8):
                    try:
                        content = future.result(timeout=1)
                        if content and len(content.strip()) > 50:
                            combined_text += content + "\n\n"
                            successful_count += 1
                            print(
                                f"‚úÖ Tr√≠ch xu·∫•t th√†nh c√¥ng {successful_count}/{len(urls)}"
                            )
                            if len(combined_text) > 5000:
                                break
                    except Exception:
                        continue
            except concurrent.futures.TimeoutError:
                print(f"‚è∞ Timeout - x·ª≠ l√Ω ƒë∆∞·ª£c {successful_count}/{len(urls)}")
                for future in future_to_url:
                    future.cancel()
        return combined_text

    def _generate_answer(self, question, context, history=""):
        """Generate answer using Gemini AI with context and history"""
        clean_context = context.replace("\n", " ").strip()
        clean_history = history.replace("\n", " ").strip() if history else ""

        prompt = f"""
            D∆∞·ªõi ƒë√¢y l√† c√°c t√†i li·ªáu tham kh·∫£o ƒë∆∞·ª£c thu th·∫≠p t·ª´ c√°c trang web uy t√≠n.
            H√£y ƒë·ªçc k·ªπ v√† ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ c√°c t√†i li·ªáu n√†y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.

            {f"L·ªãch s·ª≠ h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥: {clean_history}" if clean_history else ""}

            ---
            T√ÄI LI·ªÜU THAM KH·∫¢O:
            {clean_context}
            ---

            C√ÇU H·ªéI: {question}

            TR·∫¢ L·ªúI:"""

        try:
            # Set up generation configuration
            generation_config = genai.GenerationConfig(
                temperature=0.1,
                max_output_tokens=800,
                top_p=0.95,
                top_k=40,
            )

            # Set safety settings
            safety_settings = {
                "HARM_CATEGORY_HARASSMENT": "BLOCK_MEDIUM_AND_ABOVE",
                "HARM_CATEGORY_HATE_SPEECH": "BLOCK_MEDIUM_AND_ABOVE",
                "HARM_CATEGORY_SEXUALLY_EXPLICIT": "BLOCK_MEDIUM_AND_ABOVE",
                "HARM_CATEGORY_DANGEROUS_CONTENT": "BLOCK_MEDIUM_AND_ABOVE",
            }

            # Generate content using the model
            response = self.model.generate_content(
                contents=prompt,
                generation_config=generation_config,
                # safety_settings=safety_settings,
            )
            return response.text.strip()
        except Exception as e:
            return f"L·ªói khi g·ªçi AI Gemini: {e}"

    def _rag_process(self, question, history=""):
        """Complete RAG process: search, extract, generate - TURBO MODE"""
        start_time = time.time()

        print("üîç T√¨m ki·∫øm...")
        search_start = time.time()
        urls = self._search_duckduckgo(question)
        print(f"   ‚ö° Search: {time.time() - search_start:.1f}s")

        if not urls:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin."

        print(f"üìÑ X·ª≠ l√Ω {len(urls)} trang...")
        extract_start = time.time()
        combined_text = self._process_urls_parallel(urls)
        print(f"   ‚ö° Extract: {time.time() - extract_start:.1f}s")

        if not combined_text.strip():
            return "Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c th√¥ng tin."

        print(f"üìù Thu th·∫≠p {len(combined_text)} k√Ω t·ª±")
        print("ü§ñ T·∫°o c√¢u tr·∫£ l·ªùi...")
        ai_start = time.time()
        answer = self._generate_answer(question, combined_text, history)
        print(f"   ‚ö° AI: {time.time() - ai_start:.1f}s")

        total_time = time.time() - start_time
        print(f"üöÄ T·ªîNG: {total_time:.1f}s")

        return answer

    def chat(self, question):
        """Single chat interaction with history context"""
        print("\n" + "=" * 50)
        print(f"üë§ B·∫°n: {question}")
        print("=" * 50)

        # Create context from history
        history_context = ""
        if self.history:
            recent_history = self.history[-self.max_history :]
            history_context = " ".join(
                [f"Q: {h['q']} A: {h['a']}" for h in recent_history]
            )

        # Get answer
        answer = self._rag_process(question, history_context)

        # Save to history
        self.history.append({"q": question, "a": answer})

        print(f"\nüé≠ H∆∞·ªõng d·∫´n vi√™n Hu·∫ø: {answer}")
        return answer

    def start(self):
        """Start interactive chatbot session"""
        print("üèõÔ∏è Ch√†o m·ª´ng ƒë·∫øn v·ªõi Chatbot VƒÉn h√≥a Hu·∫ø!")
        print("üí¨ H·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ vƒÉn h√≥a, l·ªãch s·ª≠, ·∫©m th·ª±c Hu·∫ø...")
        print("‚õî G√µ 'quit' ho·∫∑c 'tho√°t' ƒë·ªÉ k·∫øt th√∫c\n")

        while True:
            try:
                question = input("üé§ H·ªèi v·ªÅ Hu·∫ø: ").strip()

                if question.lower() in ["quit", "exit", "tho√°t", "q"]:
                    print("\nüëã T·∫°m bi·ªát! Ch√∫c b·∫°n c√≥ chuy·∫øn du l·ªãch Hu·∫ø th√∫ v·ªã!")
                    break

                if not question:
                    continue

                self.chat(question)

            except KeyboardInterrupt:
                print("\n\nüëã T·∫°m bi·ªát!")
                break
            except Exception as e:
                print(f"‚ùå L·ªói: {e}")


# Initialize and start chatbot
if __name__ == "__main__":
    print("üöÄ Kh·ªüi ƒë·ªông chatbot...")
    chatbot = HueChatbot()

    # Start interactive session
    chatbot.start()
