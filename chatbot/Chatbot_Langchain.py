import os
from dotenv import load_dotenv

# --- LangChain Imports ---
# Core components for building chains
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory

# LLM integration with Google Gemini
from langchain_google_genai import ChatGoogleGenerativeAI

# Pre-built tool for web search
from langchain_community.tools import DuckDuckGoSearchRun

# In-memory history for the chat session
from langchain.memory import ChatMessageHistory


class HueChatbotLangChain:
    def __init__(self):
        """Initialize the chatbot using LangChain components."""
        load_dotenv()
        # API Key is automatically picked up by the LangChain integration
        # if GOOGLE_API_KEY is set in the environment.
        if not os.getenv("GOOGLE_API_KEY"):
            # LangChain uses GOOGLE_API_KEY, so let's ensure it's set
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError(
                    "API key not found. Please set GOOGLE_API_KEY or GEMINI_API_KEY."
                )
            os.environ["GOOGLE_API_KEY"] = api_key

        # --- 1. Create the "lego blocks" of LangChain ---

        # a. LLM: The "brain" of the operation.
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0,
        )

        # b. Retriever: The "information gatherer".
        self.search_tool = DuckDuckGoSearchRun(
            region="vn",
            language="vi",
            max_results=5,
        )

        # c. Prompt Template: The "instruction manual" for the LLM.
        self.prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """
                    Báº¡n lÃ  hÆ°á»›ng dáº«n viÃªn du lá»‹ch chuyÃªn nghiá»‡p vá» vÄƒn hÃ³a Huáº¿. 
                    HÃ£y tráº£ lá»i cÃ¡c cÃ¢u há»i má»™t cÃ¡ch tá»± nhiÃªn, thÃ¢n thiá»‡n vÃ  chÃ­nh xÃ¡c dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p trong tÃ i liá»‡u tham kháº£o. 
                    LuÃ´n giá»¯ vai trÃ² lÃ  má»™t chuyÃªn gia am hiá»ƒu vá» Huáº¿, loáº¡i bá» cÃ¡c thÃ´ng tin thÆ°Æ¡ng máº¡i, bÃ¡n hÃ ng giÃ¡ cáº£ náº¿u ngÆ°á»i dÃ¹ng khÃ´ng yÃªu cáº§u.
                    Tráº£ lá»i ngáº¯n gá»n, sÃºc tÃ­ch vÃ  táº­p trung vÃ o vÄƒn hÃ³a, lá»‹ch sá»­, áº©m thá»±c vÃ  cÃ¡c Ä‘iá»ƒm du lá»‹ch ná»•i tiáº¿ng cá»§a Huáº¿,
                    dá»±a trÃªn lá»‹ch sá»­ trÃ² chuyá»‡n vÃ  ngá»¯ cáº£nh tÃ¬m kiáº¿m dÆ°á»›i Ä‘Ã¢y.\n\n"
                    "Ngá»¯ cáº£nh tÃ¬m kiáº¿m:\n{context}",
                    """,
                ),
                ("human", "{input}"),
            ]
        )

        # d. Output Parser: Cleans up the final response.
        self.output_parser = StrOutputParser()

        # --- 2. Chaining the components into a runnable chain ---

        base_chain = (
            RunnablePassthrough.assign(
                context=lambda x: self.search_tool.run(
                    f"{x['input']} vÄƒn hÃ³a Huáº¿",
                )
            )
            | self.prompt
            | self.llm
            | self.output_parser
        )

        # --- 3. Storing chat history ---
        self.chat_history_store = {}

        # Using RunnableWithMessageHistory to manage chat history
        # This allows us to keep track of the conversation context.
        # It will automatically handle the input and history messages.
        # The session_id is used to differentiate between different chat sessions.

        self.conversational_chain = RunnableWithMessageHistory(
            base_chain,
            lambda session_id: self.chat_history_store.get(
                session_id, ChatMessageHistory()
            ),
            input_messages_key="input",
            history_messages_key="chat_history",
        )

    def chat(self, question, session_id="default_session", streaming=False):
        """Single chat interaction handled by the LangChain chain."""
        print("\n" + "=" * 50)
        print(f"ğŸ‘¤ Báº¡n: {question}")
        print("=" * 50)

        print("ğŸ¤– HÆ°á»›ng dáº«n viÃªn Huáº¿ (sá»­ dá»¥ng LangChain) Ä‘ang suy nghÄ©...")

        # Create a new chat history if it doesn't exist
        if session_id not in self.chat_history_store:
            self.chat_history_store[session_id] = ChatMessageHistory()

        if streaming:
            for chunk in self.conversational_chain.stream(
                {"input": question}, config={"configurable": {"session_id": session_id}}
            ):
                print(chunk, end="", flush=True)

            return None

        answer = self.conversational_chain.invoke(
            {"input": question}, config={"configurable": {"session_id": session_id}}
        )

        print(f"\nğŸ­ HÆ°á»›ng dáº«n viÃªn Huáº¿: {answer}")
        return answer

    def start(self, streaming=False):
        """Start interactive chatbot session."""
        print("ğŸ›ï¸ ChÃ o má»«ng Ä‘áº¿n vá»›i Chatbot VÄƒn hÃ³a Huáº¿! (PhiÃªn báº£n LangChain)")
        print("ğŸ’¬ Há»i tÃ´i báº¥t cá»© Ä‘iá»u gÃ¬ vá» vÄƒn hÃ³a, lá»‹ch sá»­, áº©m thá»±c Huáº¿...")
        print("â›” GÃµ 'quit' hoáº·c 'thoÃ¡t' Ä‘á»ƒ káº¿t thÃºc\n")

        session_id = "main_chat_session"

        while True:
            try:
                question = input("ğŸ¤ Há»i vá» Huáº¿: ").strip()
                if question.lower() in ["quit", "exit", "thoÃ¡t", "q"]:
                    print("\nğŸ‘‹ Táº¡m biá»‡t! ChÃºc báº¡n cÃ³ chuyáº¿n du lá»‹ch Huáº¿ thÃº vá»‹!")
                    break
                if not question:
                    continue
                self.chat(question, session_id=session_id, streaming=streaming)
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Táº¡m biá»‡t!")
                break
            except Exception as e:
                print(f"âŒ Lá»—i: {e}")


# Initialize and start chatbot
if __name__ == "__main__":
    print("ğŸš€ Khá»Ÿi Ä‘á»™ng chatbot...")
    chatbot = HueChatbotLangChain()
    chatbot.start(streaming=False)
